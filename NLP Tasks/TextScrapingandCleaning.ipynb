{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4bbf15",
   "metadata": {},
   "source": [
    "# Text Scraping and Cleaning\n",
    "Text scraping is an incredibly common application of Natural Language Processing. **Text scraping**, sometimes called web scraping, extracts text from websites or other online sources. It involves automatically retrieving web pages, parsing the HTML content, and extracting relevant text for further analysis. \n",
    "\n",
    "Text scraping is a common technique used to gather large amounts of textual data from the web, which can then be used for various NLP tasks such as sentiment analysis, topic modeling, language modeling, and more. By extracting text from websites, researchers and data scientists can obtain valuable data for training and testing NLP models or conducting text-based research\n",
    "\n",
    "Typically, text scraping involves the following steps:\n",
    "\n",
    "1. Retrieving web pages: The scraping process starts with fetching the HTML content of web pages. This can be done using libraries or frameworks like Beautiful Soup, Scrapy, or Selenium, which provide tools for interacting with web pages programmatically.\n",
    "\n",
    "2. Parsing HTML content: Once the web page is retrieved, the HTML content needs to be parsed to identify the relevant text elements. HTML parsing libraries like Beautiful Soup or lxml can be used to navigate the HTML structure and extract specific text elements or classes.\n",
    "\n",
    "3. Extracting text: After identifying the relevant HTML elements, the desired text is extracted from them. This may involve extracting the text from HTML tags, cleaning the text by removing unwanted characters or HTML markup, and organizing the extracted text in a suitable format.\n",
    "\n",
    "Storing the extracted data: Finally, the extracted text can be stored in a structured format such as CSV, JSON, or a database for further analysis or integration into NLP workflows.\n",
    "\n",
    "It's important to note that when performing text scraping, one should respect website owners' terms of service, adhere to legal and ethical guidelines, and be mindful of not overloading servers with excessive requests. Additionally, some websites may have restrictions on scraping or may require authentication for access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998953b",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "There are several ways to scrape web content from the internet. Here are some common methods:\n",
    "\n",
    "1. Libraries and frameworks: Various programming libraries and frameworks provide functionality for web scraping. Some popular options include:\n",
    "\n",
    "  -  **Beautiful Soup**: A Python library for parsing HTML and XML documents, making it easier to navigate and extract data from web pages.\n",
    "  - **Scrapy**: A Python framework for building web spiders, which are specialized programs for automated web scraping.\n",
    "  -  **Selenium**: A tool for automating web browsers, useful for scraping web pages that require JavaScript execution or user interaction.\n",
    "  \n",
    "2. API access: Many websites and online services provide APIs (Application Programming Interfaces) that allow you to access and retrieve data in a structured manner. APIs often provide specific endpoints or methods for retrieving data, which can be more reliable and efficient compared to scraping HTML directly. However, not all websites offer APIs, and some may have restrictions or require authentication.\n",
    "\n",
    "3. RSS feeds: Some websites publish RSS (Really Simple Syndication) feeds that provide structured updates or summaries of their content. RSS feeds can be parsed to extract relevant text or metadata, making it easier to gather specific information from multiple sources.\n",
    "\n",
    "4. Data marketplaces: There are data marketplaces and platforms that provide pre-scraped data from various websites, saving you the effort of scraping yourself. These platforms often offer APIs or downloadable datasets for easy integration into your projects.\n",
    "\n",
    "5. Custom scripts: For websites without APIs or other scraping-friendly features, you can write custom scripts using programming languages like Python, Ruby, or JavaScript. These scripts typically simulate user interactions, such as sending HTTP requests, parsing HTML responses, and extracting desired content.\n",
    "\n",
    "6. Scraping tools and services: Several tools and services are available that simplify the process of web scraping. They offer point-and-click interfaces, require little to no coding, and provide features like scheduling, data extraction, and storage. Examples include Octoparse, ParseHub, and Import.io.\n",
    "\n",
    "\n",
    "We'll start by looking at two python libraries that can scrape Wikipedia for data: `wikipedia` and `beautiful soup`. In addition to these two libraries, we'll leverage `spacy` for some additional processing of the scaped text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9431a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kaila\\tensorprojects\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b89682",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import package\n",
    "import wikipedia\n",
    "\n",
    "# Specify the title of the Wikipedia page\n",
    "wiki = wikipedia.page(\"Tiger Woods\")\n",
    "\n",
    "# Extract the plain text content of the page\n",
    "text = wiki.content\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e77536",
   "metadata": {},
   "source": [
    "Now that we've used the `wikipedia` library to pull data into memory, let's use the `regular expressions` library, `re` to do some string manipulation to clean the data. The Python regular expression library, `re`, provides powerful tools for pattern matching and manipulation of text strings. It allows developers to search, match, and manipulate strings based on specified patterns using a combination of metacharacters and special sequences. With the `re` library, you can perform tasks like extracting specific information from text, validating input patterns, or replacing text based on certain criteria, making it a valuable tool for text processing and data extraction tasks. Read more about the library [here](https://docs.python.org/3/library/re.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038affbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import re\n",
    "# Clean text\n",
    "text = re.sub(r'==.*?==+', '', text)\n",
    "text = text.replace('\\n', '')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c016b",
   "metadata": {},
   "source": [
    "As an alternative to the `re` library, the `BeautifulSoup` library can be used to scrape the web. `BeautifulSoup` is a Python library used for parsing HTML and XML documents. It provides an easy-to-use interface for navigating and manipulating the parsed data, making it convenient for web scraping tasks. With Beautiful Soup, developers can extract specific elements, search for tags or attributes, and navigate the document tree structure, simplifying the process of extracting desired information from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url of the web page\n",
    "source = urlopen('https://en.wikipedia.org/wiki/Tiger_Woods').read()\n",
    "\n",
    "# Make a soup \n",
    "soup = BeautifulSoup(source,'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d6aec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(set([text.parent.name for text in soup.find_all(text=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fdce8",
   "metadata": {},
   "source": [
    "Next, we can extract the plain text content from the paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the plain text content from paragraphs\n",
    "text = ''\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506042b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "text = re.sub(r'\\[.*?\\]+', '', text)\n",
    "text = text.replace('\\n', '')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7f20f",
   "metadata": {},
   "source": [
    "## Using `spaCy` for Cleaning\n",
    "\n",
    "`spaCy` is a powerful Python library for natural language processing (NLP) tasks. It provides efficient tools and models for tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and more. Spacy's focus on speed and efficiency makes it suitable for processing large amounts of text data and building scalable NLP pipelines.\n",
    "\n",
    "In order to use various spacy models, you'll need to download them. Please run the following snippets of code in your local terminal app to download the models:\n",
    "- `python -m spacy download en_core_web_sm`\n",
    "- `python3 -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Import the english language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0912a68",
   "metadata": {},
   "source": [
    "For this example, we'll use some sample wikipedia data about various people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/supplementary_content/people_wiki.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f49f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7fa91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select a single record \n",
    "txt = df[\"text\"][1111]\n",
    "txt[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146dc83f",
   "metadata": {},
   "source": [
    "Using this sample text, we'll do the following:\n",
    "- Use the `nlp()` function from the Spacy library is used to process the text and create a doc object, which represents the processed document.\n",
    "- A list called olist is initialized to store the information of each token in the document. The code then iterates over each token in the doc object using a for loop. For each token, various properties such as the text itself (token.text), starting index (token.idx), lemma (base form of the token, token.lemma_), punctuation status (token.is_punct), space status (token.is_space), shape (token.shape_), part of speech (token.pos_), and POS tag (token.tag_) are extracted and stored in a list\n",
    "- After processing all tokens, the olist is converted into a pandas DataFrame. The column names of the DataFrame are then set accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)    \n",
    "olist = []\n",
    "for token in doc:\n",
    "    l = [token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_]\n",
    "    olist.append(l)\n",
    "    \n",
    "odf = pd.DataFrame(olist)\n",
    "odf.columns= [\"Text\", \"StartIndex\", \"Lemma\", \"IsPunctuation\", \"IsSpace\", \"WordShape\", \"PartOfSpeech\", \"POSTag\"]\n",
    "odf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6961fce",
   "metadata": {},
   "source": [
    "This code iterates over each named entity (ent) in the `doc.ents` attribute, which contains a list of named entities recognized by `spaCy`.\n",
    "\n",
    "For each named entity, the code extracts the text of the entity (`ent.text`) and its label (`ent.label_`), which represents the type or category of the named entity. The text and entity type are appended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1c26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(txt)\n",
    "olist = []\n",
    "for ent in doc.ents:\n",
    "    olist.append([ent.text, ent.label_])\n",
    "    \n",
    "odf = pd.DataFrame(olist)\n",
    "odf.columns = [\"Text\", \"EntityType\"]\n",
    "odf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ba8f8",
   "metadata": {},
   "source": [
    "The library also includes `display` functionality to better display text with named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1be3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = df[\"text\"][3003]\n",
    "doc = nlp(txt)\n",
    "colors = {'GPE': 'lightblue', 'NORP':'lightgreen'}\n",
    "options = {'ents': ['GPE', 'NORP'], 'colors': colors}\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca177e8",
   "metadata": {},
   "source": [
    "Noun chunks are \"base noun phrases\" â€“ flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun. We'll look at how to do noun phrase chunking using `spaCy`. In addition to noun phrase chunking, `spaCy` also gets us the root of the noun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00626823",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)\n",
    "olist = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    olist.append([chunk.text, chunk.label_, chunk.root.text])\n",
    "odf = pd.DataFrame(olist)\n",
    "odf.columns = [\"NounPhrase\", \"Label\", \"RootWord\"]\n",
    "odf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fa3de",
   "metadata": {},
   "source": [
    "In addition to noun counts, we can use `spaCy` to parse through different levels of text, referred to as a **dependency parser**. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f75d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df[\"text\"][1009])\n",
    "olist = []\n",
    "for token in doc:\n",
    "    olist.append([token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children]])\n",
    "odf = pd.DataFrame(olist)\n",
    "odf.columns = [\"Text\", \"Dep\", \"Head text\", \"Head POS\", \"Children\"]\n",
    "odf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491656e8",
   "metadata": {},
   "source": [
    "A **syntactic dependency** tree visualization is a graphical representation of the syntactic structure and relationships among words in a sentence or text. It represents how words are connected to each other based on their grammatical roles and dependencies.\n",
    "\n",
    "In a dependency tree, each word in the sentence is represented as a node, and the relationships between words are represented as labeled arcs or edges. The arcs indicate the grammatical dependencies, such as subject-verb, verb-object, or modifier relationships.\n",
    "\n",
    "The visualization can provide valuable insights into the grammatical structure of a sentence, highlighting the dependencies between words and how they contribute to the overall meaning of the sentence. It can be useful for understanding and analyzing sentence structure, parsing, part-of-speech tagging, and other linguistic tasks.\n",
    "\n",
    "\n",
    "`spaCy` can display a syntactic dependency tree visualization of the document using the 'dep' style with specified options. Other style options can be configured, read more about it [here](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb50960",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6bc34",
   "metadata": {},
   "source": [
    "## Word Similarity \n",
    "Word similarity refers to the measurement of the degree of semantic or contextual similarity between two or more words. It involves quantifying the likeness or relatedness between words based on their meaning, context, or usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8186082",
   "metadata": {},
   "source": [
    "We'll calculate the `cosine similarity` as the spacial distance between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07145563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "queen = nlp.vocab[\"software\"].vector\n",
    "computed_similarities = []\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    "    similarity = cosine_similarity(queen, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8f02c",
   "metadata": {},
   "source": [
    "Now, that we've calculated cosine similarity let's check to see how words relate based on the sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "software = nlp.vocab[\"software\"]\n",
    "computer = nlp.vocab[\"computer\"]\n",
    "silicon_valley = nlp.vocab[\"Silicon Valley\"]\n",
    "unix = nlp.vocab[\"unix\"]\n",
    "microsystems = nlp.vocab[\"microsystems\"]\n",
    " \n",
    "print(\"Word similarity score between software and computer : \",software.similarity(computer))\n",
    "print(\"Word similarity score between software and Silicon Valley : \",software.similarity(silicon_valley))\n",
    "print(\"Word similarity score between software and unix : \",software.similarity(unix))\n",
    "print(\"Word similarity score between software and microsystems : \",software.similarity(microsystems))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
