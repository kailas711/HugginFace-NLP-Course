{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fdc925",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "**Topic modeling** is a technique in natural language processing (NLP) that aims to identify hidden thematic structures or topics within a collection of documents. It is an unsupervised machine learning approach that automatically clusters words or documents together based on their co-occurrence patterns, statistical distributions, or semantic similarities.\n",
    "\n",
    "The goal of topic modeling is to discover latent topics that can explain the main themes or subjects present in the text data. Each topic is represented as a probability distribution over words, indicating the likelihood of certain words appearing in that topic. Documents are then represented as a mixture of these topics, indicating the proportion of each topic present in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95227d88",
   "metadata": {},
   "source": [
    "Let's read in the wikipedia data we've used in previous noteoboks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/supplementary_content/people_wiki.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06654234",
   "metadata": {},
   "source": [
    "We'll define a few functions that will help us clean the text and remove any stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ad8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Function that cleans up text data using various regular expression patterns\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6049c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords using the NLTK English stopwords.\n",
    "    \"\"\"\n",
    "    my_stopwords = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)  # tokenize \n",
    "    tokens = [t for t in tokens if not t in my_stopwords]  # Remove stopwords\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to our text data\n",
    "df[\"clean_text\"] = df.text.apply(clean)\n",
    "df[\"tokens\"] = df.clean_text.apply(remove_stopwords_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a990d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly visualize \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c5f0c",
   "metadata": {},
   "source": [
    "## Apply LDA \n",
    "One popular topic modeling algorithm is **Latent Dirichlet Allocation (LDA)**, which assumes that each document is a mixture of topics, and each topic is a mixture of words. LDA identifies the latent topics and their corresponding word distributions by iteratively learning the topic-word and document-topic distributions. We'll use the `genism` library as the python interface to apply LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(df[\"tokens\"])\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df[\"tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6239fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, random_state=100,\n",
    "                chunksize=200, passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topic_table(lda_model, corpus, texts=df):\n",
    "    \"\"\"\n",
    "    Generates document topic table \n",
    "    \n",
    "    @params\n",
    "    lda_model: gensim.models.ldamodel.LdaModel,\n",
    "    corpus: list, document corpus\n",
    "    texts: pd.DataFrame\n",
    "    \n",
    "    @returns \n",
    "    pd.DataFrame: returns topic keywords for each document\n",
    "    \"\"\"\n",
    "    # Init output\n",
    "    document_topic_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(lda_model[corpus]):           \n",
    "        row = sorted(row_list, key=lambda x: (x[1]), reverse=True)\n",
    "        topic_num=row[0][0]\n",
    "        prop_topic=row[0][1]\n",
    "        wp = lda_model.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        document_topic_df.at[i,'best_topic'] = topic_num\n",
    "        document_topic_df.at[i,'prop_topic'] = prop_topic\n",
    "        document_topic_df.at[i,'topic_keywords'] = topic_keywords\n",
    "        document_topic_df.at[i,'document_num'] = i\n",
    "    return document_topic_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bb256",
   "metadata": {},
   "source": [
    "We'll create a document table that highlights the `best_topic`, `prop_topic`, `topic_keywords`, and a `document_num` as a document indentifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e4796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_topic_df = get_document_topic_table(lda_model=lda_model, corpus=corpus, texts=df[\"tokens\"])\n",
    "document_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42f010",
   "metadata": {},
   "source": [
    "Now that we have our LDA model and our documents table, we can write a few functions that will aggregate and return the `k` most related topics. In our example, a `topic` represents a `person` from the wikipedia dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_id(doc_id):\n",
    "    \"\"\"\n",
    "    Get the id associated with select topic\n",
    "    \n",
    "    @params:\n",
    "    doc_id: str, document identifier\n",
    "    \n",
    "    @returns:\n",
    "    pd.DataFrame: matched topics\n",
    "    \"\"\"\n",
    "    for i,row in df.iterrows():\n",
    "        if(row[\"URI\"]==doc_id):\n",
    "            return document_topic_df[\"best_topic\"][i]\n",
    "    return -1\n",
    "\n",
    "def get_matching_topics_docs(topic_id):\n",
    "    \"\"\"\n",
    "    Gets the matching topic documents \n",
    "    \n",
    "    @params:\n",
    "    topic_id: str, lookup \n",
    "    \n",
    "    @returns:\n",
    "    matched_topics: list, list of matched topics \n",
    "    \"\"\"\n",
    "    matched_topics=[]\n",
    "    for i,row in document_topic_df.iterrows():\n",
    "        \n",
    "        if(row[\"best_topic\"]==topic_id):\n",
    "            topic_prop_doc=(topic_id,row[\"prop_topic\"],i)\n",
    "            matched_topics.append(topic_prop_doc)\n",
    "        \n",
    "    return matched_topics\n",
    "\n",
    "def get_top_k_topics(matched_topics,k):\n",
    "    \"\"\"\n",
    "    Getting matched K topics \n",
    "    \n",
    "    @params:\n",
    "    matched_topics:list of matched toipcs\n",
    "    k: int, top K related topics\n",
    "    \n",
    "    @returns:\n",
    "    k_topics_df: pd.DataFrame of matched topics\n",
    "    \"\"\"\n",
    "    top_k=sorted(matched_topics, key=lambda x: [x[1]], reverse=True)\n",
    "    k_topics_df=pd.DataFrame(columns=[\"doc_id\",\"topic_id\",\"topic_prop\",\"title\"])\n",
    "    i=0\n",
    "    for topic_id,topic_prop,doc_num in top_k[:k]:\n",
    "        k_topics_df.at[i,'doc_id']=df[\"URI\"][doc_num]\n",
    "        k_topics_df.at[i,'topic_id']=topic_id\n",
    "        k_topics_df.at[i,'topic_prop']=topic_prop\n",
    "        k_topics_df.at[i,'title']=df[\"name\"][doc_num]\n",
    "        i+=1\n",
    "    return k_topics_df\n",
    "\n",
    "def recommend_k_topics(doc_id,k):\n",
    "    \"\"\"\n",
    "    Identifies topics, gets list of K most simliar topics \n",
    "    \n",
    "    @params:\n",
    "    doc_id: str document id, \n",
    "    k: int number of matched topics to return\n",
    "    \n",
    "    @returns:\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    topic_id=get_topic_id(doc_id)\n",
    "    if(topic_id!=-1):\n",
    "        matched_topics=get_matching_topics_docs(topic_id) \n",
    "        return get_top_k_topics(matched_topics,k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b130025",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_topics_df=recommend_k_topics(doc_id=\"<http://dbpedia.org/resource/Alfred_J._Lewy>\",k=10)\n",
    "k_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a303a2",
   "metadata": {},
   "source": [
    "Note that for the sake of example, we only trained our LDA model for one pass. In more real-life applications, that number would be much higher and the model will be a bettter at topic modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
