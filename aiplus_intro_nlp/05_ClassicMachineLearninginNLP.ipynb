{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b51a945",
   "metadata": {},
   "source": [
    "# Machine Learning for NLP\n",
    "\n",
    "\n",
    "Up to this point in the learning guide, only text preprocessing and embeddings have been discussed.  Remember that the output of the embedding step is a numeric vector that can now be used for machine learning. Once embeddings are generated, the next step is to select and train models according to the task at hand. Several factors go into the model selection process including:\n",
    "- Desired output / task \n",
    "- Volume of data \n",
    "- Computational resources available\n",
    "- Availability of labels \n",
    "- Data’s domain area \n",
    "- Training or serving latency requirements \n",
    "- Desired model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a895010",
   "metadata": {},
   "source": [
    "## Classical Machine Learning in NLP\n",
    "\n",
    "Beginning with minimal complexity and gradually increasing complexing, traditional shallow machine learning algorithms can be used for an array of NLP tasks. Classification tasks are some of the more common tasks in NLP, including but not limited to text classification, named entity recognition (NER), and question answering. **Text classification** refers to the set of tasks that involve assigning categories to text documents. A common example is an email filter labeling emails as spam or not spam. Since this output is binary, models like a logistic regression, support vector machine, or naive bayes can be applied to generate the classification. **Named entity recognition** detects and classifies entities in text like people, places, etc. For example, NER is frequently used in healthcare to identify medical entities like diseases, drugs, and symptoms in patient records. Since NER is an instance of multi-class classification, algorithms like SVMs or naive bayes classifiers can be trained for NER tasks. Finally, another common NLP task that can leverage shallow algorithms is question answering. A model trained for **question answering** will assign probabilities to different answers depending on the question as input. Similar to NER, nearly any multiclass classification algorithm can be used for this task.  While not always the most complex approach, classical machine learning algorithms can cover a lot of ground in NLP, especially in the early stages of a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c1d52",
   "metadata": {},
   "source": [
    "![spam_filter](docs/images/spam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in spam filter example \n",
    "spam_df = pd.read_csv(\"data/emails.csv\")\n",
    "spam_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c031e6",
   "metadata": {},
   "source": [
    "Let's do some quick exploratory data analysis to identify any class imbalance. This will be important when evaluating the model down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df[\"spam\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782241f9",
   "metadata": {},
   "source": [
    "From these `value_counts()`, it's clear that there is some imbalance between the distribution of our labels. Depending on the modeling technique, we may need to account for this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b17594",
   "metadata": {},
   "source": [
    "Let's take a look at the average length of a spam and non-spam email. Since all text entries must be tokenized, we can write a function that tokenized the text by word and then apply it to the entire *pandas* `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `count_words` function\n",
    "def count_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95637e5b",
   "metadata": {},
   "source": [
    "Next, we'll apply the `count_work` function to count the number of words across our entire `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff888bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the `count_words` function the the entire DataFrame\n",
    "spam_df[\"counted_text\"] = spam_df[\"text\"].apply(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60d0aa",
   "metadata": {},
   "source": [
    "As a form of quick data analysis, let's take a look at the average length of an email, by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average length of an email by label\n",
    "spam_df.groupby(\"spam\")[\"counted_text\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367ecc2",
   "metadata": {},
   "source": [
    "Per the result above, it looks like there's a noteable different between the average email lenght of a spam email versus a non-spam email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc44ff",
   "metadata": {},
   "source": [
    "### Featurization \n",
    "\n",
    "Now that we've done a quick analysis of our data, we can get started with preparing the data for modeling:\n",
    "\n",
    "1. First, **stop words** are removed to ensure the remaining text carries meaning.\n",
    "2. Next, **Stemming** is applied to reduce words to their stems\n",
    "3. Lastly, **Continuous Bag of Words** algorithm is applied to generate embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15002a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffade5",
   "metadata": {},
   "source": [
    "As mentioned in previous notebooks, we'll remove stop words from our corpus. Removing these stop words ensures that the model only uses words that carry meaning and context. For this example, we'll leverage the pre-loaded English stopwords from `NLTK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43842529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove stop words\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    no_punctuation = [character for character in text if character not in string.punctuation]\n",
    "    no_punctuation = \"\".join(no_punctuation)\n",
    "    \n",
    "    return \" \".join([word for word in no_punctuation.split() if word.lower() not in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e66e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stopword removal function\n",
    "spam_df[\"removed_stopwords\"] = spam_df[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b32cd2",
   "metadata": {},
   "source": [
    "Verify stopword removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb98af",
   "metadata": {},
   "source": [
    "Now that stopwords have been removed, the next step is to apply `NLTKs` `PortStemmer` to reduce words to their stem. As a reminder, this reduction of a word to its stem allows for easier comparison of words and their respective meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5996764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25830c1",
   "metadata": {},
   "source": [
    "Define a stemming function we can apply to the entire `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db156c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stemming function\n",
    "def stem(text:str)-> str:\n",
    "    stemmer = PorterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eea17",
   "metadata": {},
   "source": [
    "Apply the stemming function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stemming function \n",
    "spam_df[\"stemming\"] = spam_df[\"removed_stopwords\"].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a88b0f",
   "metadata": {},
   "source": [
    "Verify stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df[\"stemming\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47148b1",
   "metadata": {},
   "source": [
    "Now that the corpus has been pre-processed, the last step is to apply the `CountVectorizer` that will map our text to numeric values. With this conversion, we'll be able to apply various machine learning algorithms. Per the `sklearn` documentation, here's a quick summary of the `CountVectorizer` module:\n",
    "\n",
    "```Convert a collection of text documents to a matrix of token counts.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313dbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantite, fit, and transform the `CountVectorizer`\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorized_matrix = vectorizer.fit_transform(spam_df[\"stemming\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bbdd7",
   "metadata": {},
   "source": [
    "Checking in on data types, note that the `CountVectorizer` returns a `scipy.sparse._csr.csr_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectorized_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a960ea",
   "metadata": {},
   "source": [
    "### Modeling \n",
    "\n",
    "Now that the data has been pre-processed, let's split the data and fit a model. Before fitting a model, like any other machine learing problem, the data needs to be split into a training set and a test set. As a reminder, this split ensures that our model doesn't **overfit** our data and is generalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c08318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_matrix, spam_df[\"spam\"], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e85983",
   "metadata": {},
   "source": [
    "For this example, we'll be leveraging a `Naive Bayes Classifier`. This classifier is one of the simplier classifiers. It's considered *generative* since it models the distribution of inputs for a given class or category. The model operates under the assumption that the features of the input data are conditionally independent given the class. This assumption allows the model to make predictions both quickly and accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train a Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3ea4a",
   "metadata": {},
   "source": [
    "### Generate Predictions and Evaluate the Model\n",
    "\n",
    "Evaluation of these trained models is somewhat similar to any other classification task, however there are some nuances. In general, evaluation metrics are categorized into either **intrinsic evaluation** or **extrinsic evaluation**, where intrinsic refers to the performance of a component on a defined subtask and extrinsic refers to performance of the final objective.  Extrinsic evaluation is mostly specific to the task and business context, whereas metrics like **accuracy**, **precision** and **recall**, and **F1** are considered intrinsic. Beyond traditional metrics for classification models, there are a handful of NLP specific metrics that can be helpful. **Bilingual Evaluation Understudy (BLUE)**, **METEOR**, and **ROUGE** are all metrics that evaluate the quality of text that has been translated from one language to another. These can be useful for text generation, paraphrase generation, and text summarization. **Perplexity** is another probabilistic measure that can evaluate how “confused” the model is. It measures the randomness by calculating how strong the model is at guessing the next word in a sentence. While NLP evaluation metrics often have strong overlap with classification evaluation metrics, there are a handful of NLP specific intrinsic evaluation measures that can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da848a6c",
   "metadata": {},
   "source": [
    "In the context of this model, we can leverage `sklearn`'s `classification_report` to quickly calculate several metrics at once, including `precision`, `recall`, `f1-score`, and `support`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = bayes.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34f780",
   "metadata": {},
   "source": [
    "In addition to calculating metrics, in the case of classification generating a *confusion matrix* can be helpful in visualizing how the true labels and predictions are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib\n",
    "\n",
    "plot_confusion_matrix(bayes, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e52b3",
   "metadata": {},
   "source": [
    "That's it! We've now successfully built a `Naive Bayes Classifier` on some e-mail text data! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
