{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527dd6fa",
   "metadata": {},
   "source": [
    "# Exercises \n",
    "Take the lessons from the last two notebooks and apply them to the following exercies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319d6ff",
   "metadata": {},
   "source": [
    "## Alternative Algorithms \n",
    "In the 05_Classic Machine Learning with NLP notebook, we used a Naive Bayes classifier to identify Spam emails. As an exercise, we'll ask you to implement an alternative algortithm to predict if an email is spam or not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49cd1b",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "You can reference the `05_Classic Machine Learning in NLP` notebook for tips on reading in parsing the email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "\n",
    "# import FILL_IN as pd\n",
    "\n",
    "# Read in spam filter example \n",
    "# spam_df = pd.FILL_IN(\"data/emails.csv\")\n",
    "spam_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ce05ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Subject: v - shoop  hello , welcome to the med...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Subject: you need only 15 minutes to prepare f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Subject: do i require an attorney to use this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Subject: high - quality affordable logos  corp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  spam\n",
       "0   Subject: naturally irresistible your corporate...     1\n",
       "1   Subject: the stock trading gunslinger  fanny i...     1\n",
       "2   Subject: unbelievable new homes made easy  im ...     1\n",
       "3   Subject: 4 color printing special  request add...     1\n",
       "4   Subject: do not have money , get software cds ...     1\n",
       "..                                                ...   ...\n",
       "95  Subject: v - shoop  hello , welcome to the med...     1\n",
       "96  Subject: you need only 15 minutes to prepare f...     1\n",
       "97  Subject: do i require an attorney to use this ...     1\n",
       "98  Subject: high - quality affordable logos  corp...     1\n",
       "99  Subject: save your money buy getting this thin...     1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "import pandas as pd\n",
    "\n",
    "# Read in spam filter example \n",
    "spam_df = pd.read_csv(\"data/emails.csv\")\n",
    "spam_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac5a1f",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Now that the data has been read in, we can use the `remove_stopwords` function to remove stopwords across our pandas `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac162c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "import string\n",
    "# from nltk.corpus import FILL_IN\n",
    "\n",
    "# Define function to remove stop words\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    no_punctuation = [character for character in text if character not in string.punctuation]\n",
    "    no_punctuation = \"\".join(no_punctuation)\n",
    "    \n",
    "#     return \" \".join([word for word in no_punctuation.split() if word.lower() not in FILL_IN(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f63b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define function to remove stop words\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    no_punctuation = [character for character in text if character not in string.punctuation]\n",
    "    no_punctuation = \"\".join(no_punctuation)\n",
    "    \n",
    "    return \" \".join([word for word in no_punctuation.split() if word.lower() not in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db54eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Apply the stopword removal function\n",
    "\n",
    "# spam_df[\"removed_stopwords\"] = spam_df[\"text\"].apply(FILL_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68123598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Apply the stopword removal function\n",
    "spam_df[\"removed_stopwords\"] = spam_df[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Define the stemming function\n",
    "\n",
    "# from nltk.stem import FILL_IN\n",
    "\n",
    "def stem(text:str)-> str:\n",
    "#     stemmer = FILL_IN\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0e7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Define the stemming function\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem(text:str)-> str:\n",
    "    stemmer = PorterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21026ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Apply the stemming function \n",
    "# spam_df[\"stemming\"] = spam_df[\"removed_stopwords\"].apply(FILL_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78edb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Apply the stemming function \n",
    "spam_df[\"stemming\"] = spam_df[\"removed_stopwords\"].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd7377",
   "metadata": {},
   "source": [
    "### Generating Embeddings\n",
    "\n",
    "Now that the data has been cleaned, we'll need to generate numeric representations of our text data. For these embeddings use the `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206098be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# from sklearn.feature_extraction.text import FILL_IN\n",
    "\n",
    "# Instantite, fit, and transform the `CountVectorizer`\n",
    "# vectorizer = FILL_IN\n",
    "vectorized_matrix = vectorizer.fit_transform(spam_df[\"stemming\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69556895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantite, fit, and transform the `CountVectorizer`\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_matrix = vectorizer.fit_transform(spam_df[\"stemming\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a592f",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "In modeling, it's crucial to split data into a training set and a test set to ensure the model generalizes well. Split the data into a training set and test set using a 80/20 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ffe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Split the data \n",
    "# from sklearn.model_selection import FILL_IN\n",
    "\n",
    "# X_train, X_test, y_train, y_test = FILL_IN(vectorized_matrix, spam_df[\"spam\"], test_size=FILL_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1c0e8",
   "metadata": {},
   "source": [
    "Use a classifier of choice to predict whether an email is spam or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Instantiate and train a Logistic Regressor\n",
    "# from sklearn.FILL_IN\n",
    "\n",
    "# model = FILL_IN\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090a296",
   "metadata": {},
   "source": [
    "### Generate Predictions and Evaluate the Model\n",
    "Now that the model has been trained, evaulate it using the `X_test` set from above. For a robust view, calcualate the `classification_report` and generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# from sklearn.metrics import FILL_IN\n",
    "\n",
    "# preds = model.FILL_IN(X_test)\n",
    "# print(FILL_IN(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# from sklearn.metrics import FILL_IN\n",
    "import matplotlib\n",
    "\n",
    "# FILL_IN(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409f8e6",
   "metadata": {},
   "source": [
    "## Deep Learning for NLP\n",
    "\n",
    "Use the LSTM and Data classes from the `06 Deep Learning` notebooks to create and `LSTM` model from scratch, but this time tune a few of the hyperparameters in the model to see if you can generate a better response to the knock knock joke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc63163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55f0e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "class LSTM_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        LSTM Model constructor\n",
    "        \n",
    "        @params:\n",
    "        dataset: dataset used for model training\n",
    "        \n",
    "        @returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = FILL_IN\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "#             dropout=FILL_IN,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        Forward module for training \n",
    "        \n",
    "        @params:\n",
    "        prev_state: torch.Tensor\n",
    "        \n",
    "        @returns:\n",
    "        logits: tuple\n",
    "        state: tuple\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        \"\"\"\n",
    "        Init state\n",
    "        \n",
    "        @params:\n",
    "        sequence_length: int, length of sequence\n",
    "        \n",
    "        @returns:\n",
    "        tuple\n",
    "        \n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Torch Dataset class for data loading\n",
    "    \"\"\"\n",
    "    def __init__(self,args,):\n",
    "        \"\"\"\n",
    "        Dataset class constructor\n",
    "        \n",
    "        @params:\n",
    "        args: Dict[str, Any]\n",
    "        \n",
    "        @returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        \"\"\"\n",
    "        Loading raw files modules \n",
    "        \n",
    "        @parms:\n",
    "        None\n",
    "        \n",
    "        @returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        train_df = pd.read_csv('data/dl_data/reddit-cleanjokes.csv')\n",
    "        text = train_df['Joke'].str.cat(sep=' ')\n",
    "        return text.split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        \"\"\"\n",
    "        Retrieving unique words\n",
    "        \n",
    "        @params:\n",
    "        None\n",
    "        \n",
    "        @returns:\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get length difference between word indices and sequence length\n",
    "        \n",
    "        @params:\n",
    "        None\n",
    "        \n",
    "        @return:\n",
    "        int: difference in length \n",
    "        \"\"\"\n",
    "        return len(self.words_indexes) - self.args[\"sequence_length\"]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get function \n",
    "        \n",
    "        @params:\n",
    "        index: int, index\n",
    "        \n",
    "        @returns:\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.args[\"sequence_length\"]]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.args[\"sequence_length\"]+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(dataset, model, args):\n",
    "    \"\"\"\n",
    "    Main training function \n",
    "    \n",
    "    @params:\n",
    "    dataset: torch.utils.data.Dataset\n",
    "    model: LSTM_Model\n",
    "    args: Dict[str, Any]\n",
    "    \n",
    "    @returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args[\"batch_size\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=FILL_IN)\n",
    "\n",
    "    for epoch in range(args[\"max_epochs\"]):\n",
    "        state_h, state_c = model.init_state(args[\"sequence_length\"])\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5776d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    \"\"\"\n",
    "    Generate predictions \n",
    "    \n",
    "    @params:\n",
    "    dataset: Dataset\n",
    "    model: LSTM_Model, model \n",
    "    text: str\n",
    "    next_words: int, number of next words to generate\n",
    "    \n",
    "    @returns:\n",
    "    words: list, list of words\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        \n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34cc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in code in the appropriate spots\n",
    "# Define main arguments for training\n",
    "args = {\n",
    "#     \"max_epochs\": FILL_IN, \n",
    "#     \"batch_size\": FILL_IN,\n",
    "    \"sequence_length\": 4\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = Dataset(args)\n",
    "\n",
    "# Instantiate the model \n",
    "model = LSTM_Model(dataset)\n",
    "\n",
    "# Train the model using dataset\n",
    "train(dataset, model, args)\n",
    "print(predict(dataset, model, text='Knock knock. Whos there?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
