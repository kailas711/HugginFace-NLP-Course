{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424174c5",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "**Translation** refers to the process of converting text or speech from one language to another. It involves understanding the meaning and structure of the source language and producing an equivalent expression in the target language.\n",
    "\n",
    "NLP translation systems aim to bridge the language barrier by leveraging machine learning and computational linguistics techniques. There are two primary approaches to NLP translation:\n",
    "\n",
    "- Rule-based Translation: This approach relies on linguistic rules and dictionaries to translate text. Linguists and language experts manually create these rules and mappings between words, phrases, and grammatical structures of the source and target languages. However, this method is limited by the complexity of language rules and does not handle ambiguities or context-dependent translations well.\n",
    "\n",
    "- Statistical Machine Translation (SMT): SMT is an approach that uses statistical models to learn patterns from bilingual corpora, which are large collections of translated texts. The models learn to associate phrases or sentences in the source language with their corresponding translations in the target language. They make translation decisions based on the probabilities learned from the training data. SMT systems often employ techniques like phrase-based translation and n-gram language modeling.\n",
    "\n",
    "More recently, Neural Machine Translation (NMT) has become the dominant approach for translation in NLP. NMT models are based on artificial neural networks, specifically recurrent neural networks (RNNs) or transformer models. These models learn to directly map sequences of words from the source language to the target language, considering the context and dependencies between words. NMT models have shown significant improvements in translation quality compared to traditional SMT systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fc13c",
   "metadata": {},
   "source": [
    "Note this notebooks is adapted from this [sample](https://www.kaggle.com/code/kkhandekar/machine-translation-beginner-s-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras tensorflow helper plotly tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6193906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad71f0",
   "metadata": {},
   "source": [
    "### The Data \n",
    "For this example, we are using a translation set that translates between English and French. It consists of several single words as well as more complex phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58093357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/supplementary_content/eng_-french.csv\")\n",
    "df = df.rename(columns={\"English words/sentences\":\"Eng\", \"French words/sentences\":\"Frn\" })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06459a",
   "metadata": {},
   "source": [
    "Let's quickly explore our data a little bit by looking at word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for word count\n",
    "def word_count (txt):\n",
    "    return len(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53df274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to both English an French words and phrases\n",
    "df['Eng_Count'] = df['Eng'].apply(lambda x: word_count(x))\n",
    "df['Frn_Count'] = df['Frn'].apply(lambda x: word_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55313a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( '{} English Words'.format(df['Eng_Count'].sum()) ) \n",
    "print('{} French Words'.format(df['Frn_Count'].sum()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763a728",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like any other ML problem, we'll need to preprocess out data so it's properly configured for the deep learning model. We'll leverage concepts we've discussed before tokenization and padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "# Padding Function\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8db6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize English text & determine English Vocab Size \n",
    "eng_seq, eng_tok = tokenize(df['Eng'])\n",
    "eng_vocab_size = len(eng_tok.word_index) + 1\n",
    "print(\"Complete English Vocab Size: \",eng_vocab_size)\n",
    "\n",
    "# Tokenize French text & determine French Vocab Size \n",
    "frn_seq, frn_tok = tokenize(df['Frn'])\n",
    "frn_vocab_size = len(frn_tok.word_index) + 1\n",
    "print(\"Complete French Vocab Size: \",frn_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbc9be",
   "metadata": {},
   "source": [
    "Once we've defined the various padding and tokenizing functions, we'll extract the appropriate sequences from our English and French columns respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf48d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sequence lengths \n",
    "eng_len = max([len(sentence) for sentence in eng_seq])\n",
    "frn_len = max([len(sentence) for sentence in frn_seq])\n",
    "\n",
    "print(\"English Sequence Length: \",eng_len,\"\\n\",\n",
    "      \"French Sequence Length: \",frn_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfca61",
   "metadata": {},
   "source": [
    "After apply the pre-processing, we'll split the data into a *training* set and a *test* set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data \n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Label and re-index \n",
    "# Drop Columns\n",
    "train_data = train_data.drop(columns=['Eng_Count', 'Frn_Count'],axis=1)\n",
    "test_data = test_data.drop(columns=['Eng_Count', 'Frn_Count'],axis=1)\n",
    "\n",
    "#Re-Index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d9ad4",
   "metadata": {},
   "source": [
    "Keep in mind that up to this point, we are still working with plain text (whether it be in English or French). In order to use a machine learning model, we'll need to encode this text into numeric values. In this example we are using the `Tokenizer` from `Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Tokenization --\n",
    "\n",
    "# Training Data\n",
    "train_X_seq, train_X_tok = tokenize(train_data['Eng'])\n",
    "train_Y_seq, train_Y_tok = tokenize(train_data['Frn'])\n",
    "\n",
    "train_eng_vocab = len(train_X_tok.word_index) + 1\n",
    "train_frn_vocab = len(train_Y_tok.word_index) + 1\n",
    "\n",
    "# Testing Data\n",
    "test_X_seq, test_X_tok = tokenize(test_data['Eng'])\n",
    "test_Y_seq, test_Y_tok = tokenize(test_data['Frn'])\n",
    "\n",
    "test_eng_vocab = len(test_X_tok.word_index) + 1\n",
    "test_frn_vocab = len(test_Y_tok.word_index) + 1\n",
    "\n",
    "\n",
    "# -- Padding --\n",
    "\n",
    "# Training Data\n",
    "train_X_seq = pad(train_X_seq)\n",
    "train_Y_seq = pad(train_Y_seq)\n",
    "\n",
    "# Testing Data\n",
    "test_X_seq = pad(test_X_seq)\n",
    "test_Y_seq = pad(test_Y_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45462572",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "\n",
    "In this case of SMT, we'll be using our own model that we're building from scratch. There are several existing pre-trained models that could be a good alternative to buildling a model from scratch. In the architecture, we'll be leveraging a couple `LSTM` layers to better capture context. You can read more about LTSMs for machine translation [here](https://www.kaggle.com/code/harshjain123/machine-translation-seq2seq-lstms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3575b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps, btch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, btch_size, input_length=in_timesteps, mask_zero=True))\n",
    "    \n",
    "    model.add(LSTM(btch_size))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(btch_size, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Parameters\n",
    "batch_size = 64  \n",
    "lr = 1e-3        \n",
    "\n",
    "# Instantiate model\n",
    "model = define_model(eng_vocab_size, frn_vocab_size, eng_len, frn_len, batch_size)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = Adam(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'model.h1.MT'\n",
    "epoch = 2\n",
    "val_split = 0.1\n",
    "\n",
    "# checkpoint in case something happens to interrupt training\n",
    "checkpoint = ModelCheckpoint(fn, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train model \n",
    "history = model.fit(train_X_seq, train_Y_seq,\n",
    "                    epochs=epoch, batch_size=batch_size, validation_split = val_split, callbacks=[checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the loss epoch over epoch\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.title(\"Train vs Validation - Loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376ae26",
   "metadata": {},
   "source": [
    "## Make Predictions \n",
    "\n",
    "Keep in mind that when making predictions, we've alread tokenized and embedding all of our data. This means that once we generate predictions, we'll have to decode them using the tokenizer back to plain text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions for a single word / phrase\n",
    "predictions = model.predict(test_X_seq[1:6])[0]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f46dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map embeddings back to words\n",
    "def to_text(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = ''\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The French translation is: {to_text(predictions, frn_tok)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
