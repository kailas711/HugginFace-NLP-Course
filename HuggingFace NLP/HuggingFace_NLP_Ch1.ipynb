{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "679808f6d4844e85b82ca99fad0a6126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4eb7b3dfcf44556ada7a68181635f56",
              "IPY_MODEL_6d88db253a8e4f3aa351e4524b09e669",
              "IPY_MODEL_d3865525ca2b4085b7b27591c4bd4650"
            ],
            "layout": "IPY_MODEL_21486f5b96b54912938fc666c7e247ed"
          }
        },
        "f4eb7b3dfcf44556ada7a68181635f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ebd05761c14be59d0684165451ba84",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac41287e9564d079bb6147e09b38b50",
            "value": "config.json: 100%"
          }
        },
        "6d88db253a8e4f3aa351e4524b09e669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665534d5485b4bc4b8ba44141b725b95",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bdad3afaeeb4715ab85d22d88c5628e",
            "value": 608
          }
        },
        "d3865525ca2b4085b7b27591c4bd4650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de342753559f4208a404d6c0d6df90dc",
            "placeholder": "​",
            "style": "IPY_MODEL_652e7df82e2a48048d10cb4bccbc786a",
            "value": " 608/608 [00:00&lt;00:00, 5.49kB/s]"
          }
        },
        "21486f5b96b54912938fc666c7e247ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ebd05761c14be59d0684165451ba84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac41287e9564d079bb6147e09b38b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665534d5485b4bc4b8ba44141b725b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bdad3afaeeb4715ab85d22d88c5628e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de342753559f4208a404d6c0d6df90dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "652e7df82e2a48048d10cb4bccbc786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Transformer Models\n",
        "\n",
        "NlP is a field of Linguistic and Machine Learning which deals with understanding everything related to Human Language. Not only just words but also the context of group of words.\n",
        "\n",
        "---\n",
        "\n",
        "**Common NLP Tasks**\n",
        "- Classifying each word in a sentence.\n",
        "- Classifying whole sentence.\n",
        "- Generating text content.\n",
        "- Extracting answer from text.\n",
        "- Generating a new sentence from text.\n",
        "\n",
        "The most basic object in HuggingFace is the `pipeline()` function it helps in ppreprocessing and postprocessing inputs."
      ],
      "metadata": {
        "id": "hsLd52ypkrBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier([\"I am just starting\", \"i like this course\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBAhqyNPlK4T",
        "outputId": "554c3f20-60af-4466-f773-9b3e43a16636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9993411898612976},\n",
              " {'label': 'POSITIVE', 'score': 0.9998490810394287}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Behind the scenes**\n",
        "- The text input is preprocessed into a format the model understands.\n",
        "- The preprocessed input is passed into the model.\n",
        "- The predictions of the model are postprossed.\n",
        "\n",
        "The pipeline supports several tasks including:\n",
        "\n",
        "---\n",
        "`feature-extraction, sentiment-analysis, fill-mask, question-answering, summerization, translation, zero-short-classification, text-generation`"
      ],
      "metadata": {
        "id": "sAYKD_ZipFIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero Shot Classification\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This course is about NLP using HuggingFace\",\n",
        "    candidate_labels = [\"education\", \"politics\", \"oil company\"]\n",
        ")\n",
        "\n",
        "#This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it.\n",
        "# It can directly return probability scores for any list of labels you want!\n",
        "\n",
        "# Text Generation\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model = \"distilgpt2\")\n",
        "generator(\"This course is about the development of.\",\n",
        "         max_length = 50,\n",
        "         num_return_sequences = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmEJ9CJ3omGh",
        "outputId": "0b04dd08-ff4c-45f1-fb71-90a415a62eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'This course is about NLP using HuggingFace',\n",
              " 'labels': ['education', 'politics', 'oil company'],\n",
              " 'scores': [0.9342062473297119, 0.04426104575395584, 0.021532732993364334]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Mask\n",
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This could be the best <mask> i had \", top_k = 2) #The top_k argument controls how many possibilities you want to be displayed\n",
        "\n",
        "# Question Answering\n",
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")\n",
        "\n",
        "# Translation\n",
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz1nCcSwyYfv",
        "outputId": "c2461e75-ae06-4358-aa0e-47673528c953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. How do Transformers work?\n",
        "\n",
        "*Originally designed for translation.* Broadly Transformers can be classified into 3 categories:\n",
        "1. Auto-Regressive models like GPT, LlaMa\n",
        "2. Auto-Encoder models like BERT\n",
        "3. Sequence-Sequence models like T5, BART etc.\n",
        "\n",
        "Training Large amount of raw text data in an self-supervised format, i.e letting the model figure out the labels from the data. These massive language models are then trained in a supervised fasion using transfer learning on specific downstream tasks.\n",
        "\n",
        "Predicting the next word by looking at the previous n-words is called *casual language modeling*. Another task is *Masked Language modeling* where the model learn to predict a word within a sentence.\n",
        "\n",
        "- The encoder part create a representation of input , hence it is optimized for that task\n",
        "- The decoder part generate the a target sequence given encoder representation.\n",
        "\n",
        "**Encoder only models** : Good for tasks like sentence classification, Named Entity Recognition. They are good at extraing meaning form sentences, and are often characerized as autoencoder models.\n",
        "- They hold bi-directional capabilities (context from left to right and vice aversea)\n",
        "- Intented for question answering, sequence classification , masked language modeling and NLU. Eg: BERT, RoBERTAa etc.\n",
        "- The pretraining of these models usually revolves around somehow corrupting a given sentence (by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
        "\n",
        "**Decoder only models** : Good for text generation. One can use encoder generally for all tasks of encoders but with a small loss of performance.\n",
        "- The models are unidirectional i.e at any given point the model has access to only left or right context. The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
        "\n",
        "- They are great at casual language modeling or text generation.\n",
        "Eg: GPT, GPT2 , LlaMa etc.\n",
        "\n",
        "**Encoder - Decoder models** : (Seq-Seq) tasks: Good for generative tasks that require an input like summerization or traslation. The encoder passes a numerical representation of the input (done once) then the decoder uses the representation with the input to generate outputs in an autoregressive manner.\n",
        "- Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering. Eg: T5\n",
        "\n",
        "## Behind the scenes.\n",
        "## 2.1 The Models\n",
        "\n",
        "The weights are usually loaded and stored in `~/.cache/huggingface/transformers`. You can customize your cache folder by setting the `HF_HOME` environment variable."
      ],
      "metadata": {
        "id": "0Dc60eUN1Flj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Building config\n",
        "config = BertConfig()\n",
        "\n",
        "#Building model\n",
        "model = BertModel(config)\n",
        "# Model is randomly initialized!\n",
        "model.train()\n"
      ],
      "metadata": {
        "id": "0CQOGKRXzd7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529d786a-676d-472f-d191-c52f3f8cd233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Parameters\t= 1.1B\n",
        "- Attention Variant\t= Grouped Query Attention\n",
        "- Model Size\t= Layers: 22, Heads: 32, Query Groups: 4, Embedding Size: 2048"
      ],
      "metadata": {
        "id": "gNln6bpVdiUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "model = AutoModel.from_config(config)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "679808f6d4844e85b82ca99fad0a6126",
            "f4eb7b3dfcf44556ada7a68181635f56",
            "6d88db253a8e4f3aa351e4524b09e669",
            "d3865525ca2b4085b7b27591c4bd4650",
            "21486f5b96b54912938fc666c7e247ed",
            "62ebd05761c14be59d0684165451ba84",
            "7ac41287e9564d079bb6147e09b38b50",
            "665534d5485b4bc4b8ba44141b725b95",
            "5bdad3afaeeb4715ab85d22d88c5628e",
            "de342753559f4208a404d6c0d6df90dc",
            "652e7df82e2a48048d10cb4bccbc786a"
          ]
        },
        "id": "G7e6sKK3b_sL",
        "outputId": "f495fe1b-6a9a-4a37-bd19-67c58f7bc015"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "679808f6d4844e85b82ca99fad0a6126"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaModel(\n",
              "  (embed_tokens): Embedding(32000, 2048)\n",
              "  (layers): ModuleList(\n",
              "    (0-21): 22 x LlamaDecoderLayer(\n",
              "      (self_attn): LlamaSdpaAttention(\n",
              "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (mlp): LlamaMLP(\n",
              "        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "      (input_layernorm): LlamaRMSNorm()\n",
              "      (post_attention_layernorm): LlamaRMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm): LlamaRMSNorm()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_hidden_layers=10\n",
        "config.num_attention_heads=16"
      ],
      "metadata": {
        "id": "YUQyyWjwrLKk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = AutoModel.from_config(config)\n",
        "print(sum(p.numel() for p in model.parameters()))\n",
        "print(sum(p.numel() for p in model2.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPeq_upNrcKH",
        "outputId": "1611fb31-739b-4a0a-da12-0634289cbc02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1034512384\n",
            "516466688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizers\n",
        "\n",
        "Models can olny process numbers, tokenizer breaks down sentences,words into chunks and later numbers.\n",
        "\n",
        "1. Word Based Tokenizer\n",
        "Each word is assigned an ID. The raw text is split into tokens by simple rule and it only gives decent result. There are some with extra rules for punctuation and we might end up with a large vocabulary. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the `[UNK]` or `<unk>`.\n",
        "\n",
        "2. Character Level Tokenizer\n",
        "Splits raw texts into charcters rather than words, it would be having less vocabulary size and there would be much less `<ukn>` tokens. One could argue that we'll end with with meanigless tokens while also large number of tokens too. This depends on the language.\n",
        "\n",
        "3. Sub-Word Tokens\n",
        "This approach rely on the principal that frequently used words should be split into samller subwords. This kind of splitting enable more effifent representation of long words which makes semantic meaning. Like the word \"Tokenization\" split into \"Token\" and \"ization\". This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n",
        "\n",
        "- Byte-level used in GPT.\n",
        "- Wordpeice  as used in BERT.\n",
        "- SentencePeice as used in several multilingual models."
      ],
      "metadata": {
        "id": "eoJVWppqNG44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence = \"This library is very cool to use!.\"\n",
        "tokens = tokenizer(sequence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "vz8_BpklcWK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32824c68-419a-44b7-d66b-5caf779f67a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1188, 3340, 1110, 1304, 4348, 1106, 1329, 106, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"The ids: \" ,ids)\n",
        "\n",
        "decoded_string = tokenizer.decode([101, 1188, 3340, 1110, 1304, 4348, 1106, 1329, 106, 119, 102])\n",
        "print(\"The decoded string: \",decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Qp7cgVYD9w",
        "outputId": "5f06a4a7-b149-4bd8-9955-5b220402b2b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ids:  [100, 100, 100]\n",
            "The decoded string:  [CLS] This library is very cool to use!. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Handling multiple sequences\n",
        "\n",
        "So far we've seen single sequence\n",
        "- What if we have multiple sequences??\n",
        "- Sequences of variable length??\n",
        "- Is there such a thing as too long a sequence??\n",
        "\n",
        "**Models expect a batch of inputs**"
      ],
      "metadata": {
        "id": "ce1c0_qNtASw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = [\"I've been waiting for a HuggingFace course my whole life.\", \"I like to train transformers\"]\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor([ids]) # Batching - adding an extra dimension as the trasnformer model by default takes multiple sequences.\n",
        "model(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkArS8DAYGoy",
        "outputId": "9cf95116-43f8-4b49-a1f4-b671f90913ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6099,  2.7623]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single\n",
        "sequence:\n",
        "\n",
        "`batched_ids = [ids, ids]`\n",
        "\n",
        "- Padding is to ensure that all inputs in a tokenizer have same length adding a special word called the `padding token`"
      ],
      "metadata": {
        "id": "0dOr3Ah1uaS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SflVNAC6uDin",
        "outputId": "be7ed0e6-611c-49b0-d0bd-18617a31f789"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The key feature of Transformer models is attention layers that `contextualize` each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence.\n",
        "\n",
        " Most transformer models have a sequence length of 512 or 1024, either we can use larger context length llms or truncate sequence.\n",
        ""
      ],
      "metadata": {
        "id": "n8zKWoKRvN8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ],
      "metadata": {
        "id": "MPWXf0Cpu58N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers add [CLS] and [SEP] tokens because the models are pretrained on those, some models add tokens at end only or some at begening only. Some might add it all the way.\n",
        "\n",
        "**wrap-up**"
      ],
      "metadata": {
        "id": "rY1C7MmQxxky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)"
      ],
      "metadata": {
        "id": "HVFifCw_xkXh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HHtkwVSxzkP",
        "outputId": "39bb09c0-bc58-40b6-9e6e-234c7c072332"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
              "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}